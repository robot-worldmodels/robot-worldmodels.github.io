<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <title>Robot World Models</title>
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/2.1.0/jquery.min.js"></script>
  <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/css/bootstrap.min.css" integrity="sha384-Vkoo8x4CGsO3+Hhxv8T/Q5PaXtkKtu6ug5TOeNV6gBiFeWPGFN9MuhOf23Q9Ifjh" crossorigin="anonymous">
  <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.4.1/js/bootstrap.min.js" integrity="sha384-wfSDF2E50Y2D1uUdj0O3uMBJnjuUD4Ih7YwaYd1iqfktj0Uod8GCExl3Og8ifwB6" crossorigin="anonymous"></script>
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta property="og:image" content="https://simulatingrobotworlds.github.io/images/preview.JPG" />
  <meta name="twitter:image" content="https://simulatingrobotworlds.github.io/images/preview.JPG" />
  <meta name="twitter:card" content="Robot World Models" />
  <script>
    /* JavaScript module code here */
    function localize_time(timeString) {
      var hours = timeString.split(":")[0].padStart(2, "0");
      var minutes = timeString.split(":")[1].padStart(2, "0");
      var datetime = new Date('1970-01-02T' + hours + ":" + minutes + ':00Z');
      return datetime.getHours() + ":" + datetime.getMinutes().toString().padStart(2, "0");
    }

    function adjustTimes() {
      // change timezone abbreviation in table header
      tz_abbrev = new Date().toLocaleTimeString('en-us',{timeZoneName:'short'}).split(' ')[2];
      document.querySelectorAll(".timezone-abbrev").forEach(node => node.textContent = tz_abbrev);
      // change times to local timezone
      document.querySelectorAll(".time-value").forEach(node => node.textContent = localize_time(node.textContent));

    }
  </script>
  <link href="css/style.css" rel="stylesheet">
</head>

<nav class="navbar navbar-expand-lg navbar-light bg-light" id="navbar">

  <div class="container" id="header">

    <div>
      <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbar-main" aria-controls="navbar-main" aria-expanded="false" aria-label="Toggle navigation">
        <span class="navbar-toggler-icon"></span>
      </button>
    </div> <!-- navbar-header -->

    <div class="collapse navbar-collapse" id="navbar-main">
      <ul class="navbar-nav mx-auto">
        <li class="nav-link"><a href="/index.html">About</a></li>
        <li class="nav-link"><a href="/submit.html">Call for Papers</a></li>
<!--        <li class="nav-link"><a href="/format.html">How to Participate</a></li>-->
<!--        <li class="nav-link"><a href="/livestream.html">&#127909; Livestream</a></li>-->
        <li class="nav-link"><a href="/schedule.html">Schedule</a></li>
        <li class="nav-link"><a href="/papers.html">Papers</a></li>
        <!-- <li class="nav-link"><a href="https://simulatingrobotworlds.github.io">Previous Edition</a></li> -->
        <!-- <li class="nav-link"><a href="/speakers.html">Invited Speakers</a></li> -->
        <!-- <li class="nav-link"><a href="/index.html#organizers">Organizers</a></li> -->
        <!-- <li class="nav-link"><a href="/papers.html">Papers</a></li>  -->

      </ul>

    </div> <!-- navbar-main -->

  </div>

</nav> <!-- navbar -->

<body onload="adjustTimes();">

  <div class="container" id="content">

    <div class="page-content">
<!-- 
      <div class="row justify-content-center">
        <h1 class="text-center">Workshop on Causal and Object-Centric Computer Vision for Robot Learning</h1>
        <div class="break"></div>
        <h4 class="text-center">June 17 - 18, <a href="https://iclr.cc/">CVPR 2024</a> Workshop</h4>
        <div class="break"></div>
        <hr>

      </div>  -->

      <div class="banner">
  <video src="images/l2srw2.mp4" class="banner-img" autoplay muted loop playsinline>
  <p>Your browser doesn't support HTML5 video. Here is a <a href="images/banner.mp4">link to the video</a> instead.</p>
</video>
  <div class="banner-text">
    <h2>Robot World Models</h2>
    <h4>RSS 2026 Workshop</h4>
  </div>
</div>
<div class="contributed talks">
  <p>
  </br><span class="dot-invited-talk"></span> Invited Talk,  <span class="dot-panel-discussion"></span> Panel Discussion, <span class="dot-lightning-talk"></span> Contributed Talks, <span class="dot-poster-session"></span> Poster Session,  <span class="dot-other-session"></span> Other.
  </p>
  <h2>Schedule</h2>
  <table class="table table-striped table-condensed" id="schedule">
    <thead>
      <tr>
        <th class="time">Time</th>
        <th>Speaker/Moderator</th>
        <th>Title</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td><span class="dot-other-session"></span> 08:00 - 08:15</td>
        <td>Organizers</td>
        <td>Introduction</td>
      </tr>
      <tr>
        <td><span class="dot-invited-talk"></span> 08:15 - 08:45</td>
        <td>Speaker 1</td>
        <td>Talk 1</td>
      </tr>
      <tr>
        <td><span class="dot-invited-talk"></span> 08:45 - 09:15</td>
        <td>Speaker 2</td>
        <td>Talk 2</td>
      </tr>
      <tr>
        <td><span class="dot-invited-talk"></span> 09:15 - 09:45</td>
        <td>Speaker 3</td>
        <td>Talk 3</td>
      </tr>
      <tr>
        <td><span class="dot-lightning-talk"></span> 09:45 - 10:00</td>
        <td>Contributing Speakers</td>
        <td>Lightning Talks</td>
      </tr>
      <tr>
        <td><span class="dot-poster-session"></span> 10:00 - 10:30</td>
        <td></td>
        <td>Coffee break & Poster Session 1</td>
      </tr>
      <tr>
        <td><span class="dot-invited-talk"></span> 10:30 - 11:00</td>
        <td>Speaker 4</td>
        <td>Talk 4</td>
      </tr>
            <tr>
        <td><span class="dot-invited-talk"></span> 11:00 - 11:30</td>
        <td>Speaker 5</td>
        <td>Talk 5</td>
      </tr>
      <tr>
        <td><span class="dot-panel-discussion"></span> 11:30 - 12:00</td>
        <td>Panelists</td>
        <td>Panel Discussion</td>
      </tr>
      <tr>
        <td><span class="dot-poster-session"></span> 12:00 - 12:30</td>
        <td></td>
        <td>Poster Session 2</td>
      </tr>
    </tbody>
  </table>
</div>

<!--
<div id="speakers" class="row">
  <h2>Speakers and Panelists (TBD)</h2>
  <div class="break"></div>
  <div class="container" style="max-width: 1140px;">
  <div class="row align-items-start mb-4">
  
    <div class="col-md-3 text-center">
      <img class="img-fluid rounded-circle mb-2" height="150" width="150"
        src="images/speakers/animesh-garg.jpg" alt="Animesh Garg">
      <a href="https://animesh.garg.tech/">
        <h4 class="section-heading">Animesh Garg</h4>
      </a>
      <h5 class="section-heading"><i>University of Toronto, Nvidia</i></h5>
    </div>

    <div class="col-md-9">
      <p><strong>Short Bio:</strong> Animesh Garg is the Stephen Fleming Early Career Professor in Computer Science at Georgia Tech, within the School of Interactive Computing, and is affiliated with the Robotics and Machine Learning programs. He holds courtesy appointments at the University of Toronto and the Vector Institute. Previously, he held research leadership positions at Nvidia and Apptronik. His research focuses on the algorithmic foundations of generalizable autonomy, enabling robots to acquire cognitive and dexterous skills and collaborate with humans in novel environments. His group explores structured inductive biases, causality in decision-making, multimodal object-centric representations, self-supervised learning for control, and efficient dexterous skill acquisition.</p>
   
    </div>
      <hr>

    <div class="col-md-3 text-center">
      <img class="img-fluid rounded-circle mb-2" height="150" width="150"
        src="images/speakers/daniel_ho.jpeg" alt="">
        <a href="https://itsdanielho.com/">
          <h4 class="section-heading">
            <center>Daniel Ho</center>
          </h4>
        </a>
        <h5 class="section-heading">
          <center><i>1X Technologies</i></center>
        </h5>
      </div>

    <div class="col-md-9">
      <p><strong>Short Bio:</strong> Daniel Ho is the Director of Evaluation at 1X Technologies. His goal is to deploy generalist machines that grow from experience and correct their own mistakes. He's building World Models and large-scale evaluation pipelines towards this mission. Previously, he worked on robotics, perception, and machine learning as a Senior Software Engineer at Waymo and Everyday Robots (X, Google[X]). His research has focused on learning algorithms and representation learning to generalize ML model understanding in robotics, computer vision, and self-driving.</p>
      <p><strong>Talk Title:</strong> 1X World Model: Solving humanoid policy training and evaluation with data synthesis and action control</p>
      
    </div>
      <hr>
        <div class="col-md-3 text-center">
      <img class="img-fluid rounded-circle mb-2" height="150" width="150"
        src="images/speakers/hao_su.jpg" alt="">
               <a href="https://cseweb.ucsd.edu/~haosu/">
          <h4 class="section-heading">
            <center>Hao Su</center>
          </h4>
        </a>
        <h5 class="section-heading">
          <center><i>UC San Diego</i></center>
        </h5>
      </div>

    <div class="col-md-9">
      <p><strong>Short Bio:</strong> Hao Su is an Associate Professor of Computer Science at UC San Diego and Founder & CTO of Hillbot, a robotics startup. He directs the Embodied Intelligence Lab and is a founding member of the Halıcıoğlu Data Science Institute. His research spans computer vision, machine learning, graphics, and robotics, focusing on algorithms to simulate and interact with the physical world. He holds Ph.D.s in Computer Science from Stanford and Mathematics from Beihang University. He helped develop datasets like ImageNet, ShapeNet, and tools like PointNet. Su is Program Chair of CVPR 2025 and has received NSF CAREER and SIGGRAPH awards.</p>
      <p><strong>Talk Title:</strong>Learning World Models for Embodied AI</p>

    </div>
      <hr>

    <div class="col-md-3 text-center">
      <img class="img-fluid rounded-circle mb-2" height="150" width="150"
       src="images/speakers/katerina-fragkiadaki.png" alt="">
        <a href="https://www.cs.cmu.edu/~katef/">
          <h4 class="section-heading">
            <center>Katerina Fragkiadaki</center>
          </h4>
        </a>
        <h5 class="section-heading">
          <center><i>Carnegie Mellon University</i></center>
        </h5>
      </div>

    <div class="col-md-9">
      <p><strong>Short Bio:</strong> Katerina Fragkiadaki is the JPMorgan Chase Associate Professor in Machine Learning at Carnegie Mellon University. She earned her B.S. from the National Technical University of Athens and her Ph.D. from the University of Pennsylvania, followed by postdoctoral work at UC Berkeley and Google Research. Her research combines common sense reasoning with deep visuomotor learning to enable few-shot and continual learning for perception, action, and language grounding. Her group develops methods in 2D-to-3D perception, vision-language grounding, and navigation policies. She received awards including the NSF CAREER and DARPA Young Investigator Awards and is Program Chair for ICLR 2024.</p>
   
    </div>
      <hr>

        <div class="col-md-3 text-center">
      <img class="img-fluid rounded-circle mb-2" height="150" width="150"
            src="images/speakers/yilun_du.png" alt="">
        <a href="https://yilundu.github.io/">
          <h4 class="section-heading">
            <center>Yilun Du</center>
          </h4>
        </a>
        <h5 class="section-heading">
          <center><i>Harvard University</i></center>
        </h5>
      </div>

    <div class="col-md-9">
      <p><strong>Short Bio:</strong> Yilun Du is an Assistant Professor at Harvard’s Kempner Institute and Computer Science Department, and a Senior Research Scientist at Google DeepMind. He earned his Ph.D. in EECS from MIT, advised by Leslie Kaelbling, Tomas Lozano-Perez, and Joshua Tenenbaum. He holds a bachelor’s from MIT and has been a research fellow at OpenAI and a visiting researcher at FAIR and DeepMind. A gold medalist at the International Biology Olympiad, his research focuses on generative models, decision making, robot learning, and embodied agents. He develops energy-based models enabling generalization and advances in diffusion models, scene understanding, and trajectory planning.</p>
      <p><strong>Talk Title:</strong> Learning Generative World Simulators</p>
     
    </div>
     
    </div>
  </div>
</div>
-->


    </div> <!-- container -->

  </div> <!-- page-content -->


  <div class="footnote bg-light">
    <div class="container">
      <p>For questions / comments, reach out to: <a href="mailto:	learning-to-simulate-robot-worlds@googlegroups.com">	learning-to-simulate-robot-worlds@googlegroups.com</a></p>
      <p>Website template adapted from the <a href="https://orlrworkshop.github.io/">ORLR</a>/<a href="https://oolworkshop.github.io/">OOL</a> workshops, originally based on the template of the <a href="https://baicsworkshop.github.io/">BAICS</a> workshop.</p>
    </div>
  </div>
</body>
</html>
